{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/srslogo.gif}}
Tracking of the work I am doing on a robot to compete in the next
[[http://www.robothon.org/robothon/robo-magellan.php | Seattle Robotics Society's Robo-Magellan]] event at the
[[http://www.robothon.org/robothon/index.php | Seattle Center on October 1, 2016]].



===Latest Project News!
[[https://github.com/adamgreen/Ferdinand16#project-on-pause | Project on pause for now]]\\



===Points of Interest
[[http://www.robothon.org/robothon/robo-magellan.php | SRS Robo-Magellan rules]]\\
[[https://github.com/adamgreen/Ferdinand14#readme | Notes from our 2014 attempt]]\\
[[https://github.com/adamgreen/Ferdinand16#robo-magellan-2015 | Watching Robo-Magellan 2015 from side lines]]\\
[[https://github.com/adamgreen/Ferdinand16#what-i-learned-from-robo-magellan-2015 | What I learned from watching Robo-Magellan 2015]]\\
[[https://github.com/adamgreen/Ferdinand16#clone-this-repo-and-its-submodules | How to Clone GitHub Repository]]\\



==October 4th, 2016
===Robo-Magellan 2016
The 2016 Robo-Magellan competition was held at Seattle Center as part of Robothon last Saturday, October 1st.  It was
raining at the beginning of the competition but it stopped by the time the first robot ran the course and the sun was
out by the end of the competition.

There were 4 competitors this year and what follows is the order that I believe they finished. None of the bots managed
to tag the final goal cone yet again this year.

===1st Place
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20161004-02.gif}}\\
The above animation shows Will Smith's bot tagging the bonus cone during its first run.

===2nd Place
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20161004-04.gif}}\\
The above animation shows Dave Mier's bot as it races just behind the goal cone. The animation below shows the same
bot **tagging** the bonus cone on a subsequent run.\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20161004-05.gif}}\\

===3rd Place
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20161004-03.gif}}\\
The above animation shows Albert Yen's bot as it is circles in front of the bonus cone but never quite manages to find
and tag it.

===4th Place
Bob Cook also competed today with this fine robot:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20161004-01.jpg}}\\



==September 29th, 2016
I received the [[https://github.com/adamgreen/Ferdinand16#dhb-10-serial-converter-board | DBH-10 Serial Converter PCBs]]
from OSHPark since my last posting and I decided that I would atleast get them mounted on the bot while I had a chance
even though I won't be doing much more with it this year. I soldered the 0.1" male and female headers to the board and
got it mounted on my robot. The following photos give an overview of the process:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160929-01.jpg}}\\
**Serial Converter PCB after soldering on headers**\\\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160929-02.jpg}}\\
**Serial Converter PCB mounted on DHB-10 motor controller in lower left**\\\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160929-03.jpg}}\\
**Main electronics now connected to DHB-10 via standard hobby servo cable**\\



==August 14th, 2016
===Project on Pause!
Something more important has popped up and I won't have time to complete this Robo-Magellan project in time for the
[[[[http://www.robothon.org/robothon/index.php | SRS Robothon event on October 1, 2016]]. I do plan to continue this
project for 2017.

====Where to Pickup for Robothon 2017
* Get the Arlo Robotic Platform working better out on the grass. The Arlo platform has two caster wheels: one in the
  front and another in the back. Neither of these casters are compliant to changes in the terrain so it is very easy for
  the robot to high center. I will need to correct this before I can perform more real world outdoor testing of my code.
  There is also very little clearance between the battery pack and the ground that will need to be corrected as well.
* Continue developing and testing the dead reckoning code.
* Develop and test the code to detect the orange traffic cones.
* Develop and test the obstacle advoidance code using the ultrasonic sensors on the Arlo.



==July 31st, 2016
While the month of July got off to a slow start, the progress during the last 2 weeks of the month more than made up for
it.

===Status Report
====My completed work items from last week were:
* I used tape to attach the project box (with LPC1768, Sparkfun 9DoF Sensor Stick, ESP8266 WiFi board, etc) to the Arlo
  Robotic Platform. I placed the project box in the middle of a piece of foam board (also just taped down) which had a
  hole that exactly fit the box's profile. This guarantees that I can remount the project box in the same orientation if
  I need to temporarily remove it in the near future. After some initial testing is completed, it probably makes sense
  to use aluminum or plastic hardware to more permanently attach it.\\
  {{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160728-01.jpg}}
* I re-calibrated the magnetometer after adding the ESP8266 WiFi board and mounting on the Arlo Robot Platform. I used
  the steps I [[https://github.com/adamgreen/Ferdinand14#magnetometer-calibration | previously documented here]] to
  measure the new minimum/maximum magnetometer limits and then updated them in
  [[https://github.com/adamgreen/Ferdinand16/commit/9bc8e74cfef36512c0ed7127e45e807f0620d58b#diff-1f39fe2c21e1cd7840fcc7ab6eb5f025L2 | the config.ini file.]]
  This didn't take long but required some energy as I had to rotate the 25 pound (11.3 kg) robot about each of its axis
  while running the [[https://github.com/adamgreen/Ferdinand14/tree/master/calibration | calibration utility]].
* I wrote the [[https://github.com/adamgreen/Ferdinand16/blob/master/libs/DebugSerial/DebugSerial.h | DebugSerial class]]
  to extend the capabilities of the remote debugging functionality on this bot. The remote debugging solution provided
  by [[https://github.com/adamgreen/mri#readme | my MRI debug monitor]] already had the ability to capture printf()
  output and send it to my MacBook Air to be viewed in the GDB console. The big issue with this existing approach is
  that it halts the LPC1768 CPU each time it redirects printf() output to GDB. This has a major impact on the
  performance of the robot firmware. The new DebugSerial class features a new DebugSerial::outputStringToGdb() method
  which can be used to asynchronously send output to GDB over the same serial connection using DMA to have minimal
  impact on performance.
* I [[https://github.com/adamgreen/Ferdinand16/commit/0370758c846bde5140c8017d85a9d303b4aa1f5e | added a PID control algorithm]]
  to maintain a desired heading. I am currently just using the Proportional and Integral terms of the PID algorithm and
  skipping on the Derivative term for now. This algorithm was initially used to have the robot drive slowly forward
  while maintaining the same heading as when it started execution. So far I am pretty happy with the results of this
  code (PID and Kalman filtered orientation) and the
  [[https://www.parallax.com/product/arlo-robotic-platform-system | Arlo Robot Platform]]. My PID implementation is
  heavily based on the knowledge I obtained from reading the
  [[http://controlguru.com/recognizing-integrating-non-self-regulating-process-behavior/ | Control Guru documentation on Integrating Processes]].
  I originally implemented this PID code for [[https://github.com/adamgreen/bb-8#readme | my BB-8 project]] and I only
  needed to make one modification for use in this project. The required modification was to
  [[https://github.com/adamgreen/Ferdinand16/blob/0370758c846bde5140c8017d85a9d303b4aa1f5e/firmware/robot/PID.h#L58 | constrain the angle error to be between -π and π]].\\
  {{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160730-02.gif}}
* I purchased a [[https://www.amazon.com/gp/product/B00TQEX8BO | TP-LINK TL-WR802N Wireless N300 Travel Router]] to
  enable my laptop and ESP8266 to have an access point for connection when running outside. I can power it from my
  laptop (via USB) or the power supply on the robot itself. It was easy to setup and I have had no issues using it with
  this bot so far.\\
  {{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160730-01.jpg}}



==July 25th, 2016
===DHB-10 Serial Converter Board
I decided to design a PCB board to be mounted on the 4x3 I/O headers of the
[[https://www.parallax.com/product/28231 | Parallax DHB-10 Dual Motor Controller]] so that I could utilize one standard
3-wire servo cable for the full-duplex serial connection between the DHB-10 and the LPC1768. The Parallax Propeller
board does use 3-wire servo cables for this serial connection but it needs to use two of them to support full-duplex
communications. This converter boards allows it to be done with a single cable. The board is laid out in such a way that
the user can connect the 3-wire servo cable to any of the four edges, whichever results in the best routing.\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160723-02.png}}\\
**eagle/DHB-10_Serial_Converter.brd**

|= mbed Signal |= Servo Wire Colour |= DHB-10 Pin |
| UART Tx      | White              | Ch1 - Rx |
| UART Rx      | Red                | Ch2 - Tx |
| Unconnected  | Black              | Unconnected |

I have placed an order with [[https://oshpark.com | OSH Park]] for this board and it should arrive in a few weeks. I
will continue to use jumper wires until it arrives.

===Status Report
====My completed work items from last week were:
* Interfaced the LPC1768 microcontroller to the
  [[https://www.parallax.com/product/28231 | Parallax DHB-10 Dual Motor Controller]]. I initially used the LPC1768
  and ESP8266 remote debugging/programming prototype that I already had setup on a breadboard for the development and
  testing of the [[https://github.com/adamgreen/Ferdinand16/tree/master/libs/MotorController | MotorController driver]].
* Moved the ESP8266 WiFi module and serial based motor controller connection into the project box which already
  housed the IMU hardware. A photo of this consolidation can be
  [[https://github.com/adamgreen/Ferdinand16/blob/master/photos/20160722-02.jpg | seen here]].
* Fired up the [[https://cadsoft.io | Eagle PCB Design Software]] to:
** Create a schematic to document the electronics currently found in the project box: mbed LPC1768, Sparkfun 9DoF Sensor
   Stick, Adafruit Feather HUZZAH with ESP8266, 5V power connections, etc. That schematic can be
   [[https://github.com/adamgreen/Ferdinand16/blob/master/eagle/Ferdinand16.pdf | seen here]].
** Create a layout for a PCB that allows a single 3-wire servo wire to connect to two channels on the Parallax DHB-10
   Dual Motor Controller for full-duplex serial communications whereas it currently requires two such servo wires. Three
   of these boards have been ordered from OSH Park. The layout of this PCB can be
   [[https://github.com/adamgreen/Ferdinand16/blob/master/photos/20160723-02.png | seen here]].

====My work items for the week ahead are:
* Create a temporary but robust mount for the project box on the Arlo Robotic Platform. After some initial testing is
  completed, it probably makes sense to use aluminum or plastic hardware to more permanently attach it.
* Re-calibrate the magnetometer now that the project box is mounted on Arlo.
* Start developing and testing code for dead reckoning experiments.



==July 23rd, 2016
===mbed LPC1768 and Arlo
I used free moments over the last couple of weeks to use the included
[[https://www.parallax.com/product/32912 | Parallax Propeller Activity Board WX]] to finish running through the
[[https://www.parallax.com/product/arlo-robotic-platform-system | Arlo robot platform]] experiments documented on
[[http://learn.parallax.com/tutorials/robot/arlo/arlo-activity-board-brain | this Parallax page]]. Now that the Arlo
Robotic Platform build is complete and verified, it is time for me to switch out the Propeller microcontroller board\\
{{https://github.com/adamgreen/Ferdinand16/blob/master/photos/20160720-01.jpg}}\\
and replace it with the mbed LPC1768 microcontroller board that I plan to use for this Robo-Magellan robot.\\
{{https://github.com/adamgreen/Ferdinand16/blob/master/photos/20160720-02.jpg}}

I was able to use the LPC1768 on the breadboard to start the development of the initial
[[https://github.com/adamgreen/Ferdinand16/tree/master/libs/MotorController | MotorController driver]] and put the
ESP8266 to use for WiFi based remote debugging and programming. The
[[https://github.com/adamgreen/Ferdinand16/tree/master/firmware/drive | drive sample]] was written to validate this
MotorController driver. It commands the wheels to drive ahead at a fixed rate for 2 seconds and verifies that the wheel
encoders accumulate counts as expected.

===Electronics Consolidation
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160722-01.jpg}}\\
As can be seen in the above photo, I previously had the inertial sensors and a LPC1768 microcontroller soldered inside
of a prototyping box and another LPC1768 on a breadboard with ESP8266 WiFi and DHB-10 Dual Motor Controller connections.
This left me at a point where I needed to consolidate the electronics. I soldered the ESP8266 module, motor controller
connections, and power supply connections into the prototyping box along with the inertial sensors. Once that was
completed, a single LPC1768 could be responsible for everything: IMU, dead reckoning, motor control, visual traffic cone
detection, etc. The following photo shows the current prototype after consolidation:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160722-02.jpg}}

**Notes:**
* I added a 3-pin header just below the [[https://www.sparkfun.com/products/10724 | Sparkfun 9DoF Sensor Stick]]. There
  is a blue jumper that can be used to connect the middle pin to either of its neighbors. This allows the selection of
  the voltage source for the 9DoF sensor:
** Connecting the middle pin to the pin closest to the 9DoF sensor will power the device from the USB power supply. This
   is useful when connecting the board directly to the PC for orientation sensor experimentation alone.
** Connecting the middle pin to the pin farthest away from the 9DoF sensor will power the device from the robot's 5V
   supply.
* My initial testing doesn't show the magnetometer detecting any varying magnetic interference from the ESP8266 WiFi
  device. I mounted it as far away from the 9DoF sensor stick as possible to reduce the chances of this being a problem.
  I was also careful not to route any power or signals lines underneath the 9DoF Sensor Stick.
* I added a 3-pin connector for the serial connection to the DHB-10 Dual Motor Controller. The pin closest to the DC
  Barrel Power Jack is **mbed Tx -> DHB-10 Ch1-White-Rx** and the middle pin is **DHB-10 Ch2-White-Tx -> mbed Rx**. The
  third pin is ground but I left it unconnected since the mbed and DHB-10 already share a ground through the Power
  Distribution Board.

{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160723-01.png}}\\
[[https://github.com/adamgreen/Ferdinand16/blob/master/eagle/Ferdinand16.pdf | Click to view full size PDF of the schematic]]

===ESP-LINK Firmware Downgrade
I was originally running
[[https://github.com/jeelabs/esp-link/releases/tag/v2.1.7 | esp-link v2.1.7 - 2015-12-13 08:49:46 - cb75396]] of the
firmware on the ESP8266 board and noticed two issues when running it that I didn't see with esp-link when using it with
my [[https://github.com/adamgreen/bb-8#readme | BB-8 Ball Bot]]:
* **mriprog** only achieves a rate of 10MB/sec on this robot but I was often getting >20MB/sec on the BB-8 robot.
* **GDB** would often experience timeout errors when debugging this robot. I never saw this problem on the BB-8 robot.

The BB-8 robot was running an older version of the firmware,
[[https://github.com/jeelabs/esp-link/releases/tag/v1.0.4 | esp-link v1.0.4 - 2015-09-15 21:03:54 - 2c29e29]]. I
downgraded to this version of the firmware and it solved the issues. I found that lowering the data rate from 430800 to
115200 made the debug output to GDB even more reliable. It still experiences glitches but it no longer errors out like
it did previously.



==July 18th, 2016
===IMU Updates
There were a few more changes that needed to be made to the IMU code before it would be ready for my dead reckoning
experiments. These changes include:
* I modified the orientation Processing sample's calculation of the compass heading so that it would be closer to what I
  expect my future navigation code to require:
** Return a heading angle of 0 when the robot is facing north.
** Return a heading angle of PI/2 radians when the robot is facing east and an angle of -PI/2 radians when facing west.
** Add configuration options to calibrate for the magnetic declination error at the location where the robot is being
   run and to account for how the IMU is mounted on the robot. The current settings seen in the following snippet are
   for Redmond, WA with the IMU project box mounted such that the USB cable connects towards the rear of the robot.
{{{
# This vector specifies the declination correction to be applied to
# convert magnetic north to true north.
# It can be calculated here: http://www.ngdc.noaa.gov/geomag-web/#declination
# The 3 element vector represents: degrees,minutes,seconds
compass.declinationCorrection=15,54,0

# Correct for how device is mounted on robot.
# The 3 element vector represents: degrees,minutes,seconds
compass.mountingCorrection=-90,0,0
}}}
* I ported the compass heading calculation so that it can now run on the LPC1768 to be embedded on the robot itself. The
  orientation sample contains verification code which logs an error if the compass heading it calculates from the raw
  sensor measurements mismatches what is calculated on the LPC1768.
* The LPC1768 firmware now starts up in a mode where it only returns the raw sensor measurements. This allows it to be
  used with my older [[https://github.com/adamgreen/Ferdinand14 | Ferdinand14]] Processing samples such as
  [[https://github.com/adamgreen/Ferdinand14/tree/master/calibration | the calibration utility]]. Once a reset command
  is sent over the serial connection from the PC, the Kalman filter state is initialized and the output mode is switched
  so that it now appends the orientation quaternion and the compass heading angle to the output.

===The Week Ahead
Last week was a good week for getting back into the swing of things with this Robo-Magellan bot and I want to keep the
momentum going with these items:
* Interface the LPC1768 microcontroller to the
  [[https://www.parallax.com/product/28231 | Parallax DHB-10 Dual Motor Controller]]. I initially plan to use the LPC1768
  and ESP8266 remote debugging/programming prototype that I already have setup on a breadboard.
* Move the ESP8266 WiFi module and serial based motor controller connection into the prototyping box which already
  houses the IMU hardware. Once all of the components/connections are securely soldered inside of this box, the
  development and testing of the actual dead reckoning firmware may commence.



==July 11th, 2016
It has been a few weeks since I last logged any progress on this project. I spent one week helping out on the
[[https://github.com/Smoothieware/Smoothie2 | Smoothie2 project]] with quickly completed build and debugger support
fixes. I had a few projects pop up around the house that took me away from robot building for another week. I did
receive the correct PCB mounted DC Power Jack for my
[[https://www.parallax.com/product/arlo-robotic-platform-system | Arlo robot platform]] from Parallax during the down
time, got it soldered onto the Power Distribution Board, and used it to charge the robot's SLA batteries to full
capacity. The platform is now ready for further testing and then development of the
[[https://developer.mbed.org/platforms/mbed-LPC1768/ | mbed-LPC1768]] based control system.

===June Accomplishments
I have made pretty good progress during the first month of this project:
* I ported the Kalman filter based IMU code from our
  [[https://github.com/adamgreen/Ferdinand14#readme | Robo-Magellan 2014 attempt]] so that it could run on the embedded
  LPC1768 microcontroller itself for this year's attempt. The resulting 11% CPU utilization for this port was much
  lower than I expected.
* I ordered and received the [[http://charmedlabs.com/default/pixy-cmucam5/ | Pixy camera]] with
  [[http://charmedlabs.com/default/pan-tilt-kit/ | Pan/Tilt kit]]. I still have a lot of experimentation ahead of me to
  get the most out of this important component.
* I ordered, received, and built the
  [[https://www.parallax.com/product/arlo-robotic-platform-system | Arlo robot platform]].

===July Goals
My main goals for this month include:
* Interface the LPC1768 microcontroller with the Arlo Platform. The main integration here is to develop drivers for the
  [[https://www.parallax.com/product/28231 | DHB-10 Dual H-Bridge 10 Amp Motor Controller]] used on the Arlo. In
  addition to controlling the motor rotations, it also provides wheel encoder counts which are crucial for dead
  reckoning.
* Start developing and evaluating code to run on the LPC1768 for my planned dead reckoning approach.



==June 28th, 2016
===Arlo Systems Check
I used the included [[https://www.parallax.com/product/32912 | Parallax Propeller Activity Board WX]] to perform some
basic system checks on my [[https://www.parallax.com/product/arlo-robotic-platform-system | Arlo robot platform]] over
the weekend. This was mostly just running through the checks documented on
[[http://learn.parallax.com/tutorials/robot/arlo/arlo-activity-board-brain | this Parallax page]]. They were used to
verify:
* That the motor controller could drive the motors correctly.
** To get the wheels to rotate at the expected rates, the test code needed to be updated to increase the PWM duty
   cycles. My batteries might need charging.
* That the motor controller was getting encoder counts correctly.
** Initially it wasn't getting any counts back for the left wheel. It turns out that the cables had pulled out of the
   encoders while I was completing the build. If this happens again out in the field then I will pop the motors off of
   the base to gain better access to the encoder electronics and hot glue the connectors in-place. I will also check to
   see if some of the screws need to have lock-tite applied after a few initial field runs.
* That the Ping ultrasonic sensors were working correctly. They were!

I am now waiting to hear back from Parallax about the missing DC power jack.

===Other Projects!
I plan to use this robot down time to catch up on a few work items related to the
[[https://github.com/Smoothieware/Smoothie2 | Smoothie2 project]] this week. I will jump back in on this Robo-Magellan
robot next week.



==June 24th, 2016
===Kalman Filter Fixes
After completing the port of the Kalman filter code to the LPC1768, I was experiencing floating point differences
between the orientation quaternion calculated in the original Processing code and the new LPC1768 port. Earlier this
week I investigated the problem and found a few bugs that contributed to these differences:
* The C/C++ code on the LPC1768 to synchronize the resetting of the Kalman filter wasn't completely thread safe. It was
  possible for it to signal the PC that it had reset the filter when it actually hadn't due to a concurrency bug between
  main() and serialReceiveISR().
* The Processing code on the PC should only check for floating point differences in the calculated orientation
  quaternions when in Embedded mode and the reset of the Kalman filter has completed on both sides (PC and LPC1768).
* When porting the Kalman filter code to the LPC1768, I noticed that the final quaternion result wasn't being normalized
  and I corrected it. I had forgotten to go back and make the same change in the Processing code.

Once I had fixed those bugs I no longer experienced floating point differences that exceed the 1.0E-5 threshold. The two
versions of the Kalman filter now appear to perform exactly the same.

===Arlo Under Construction
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160624-01.jpg}}\\
I started putting my [[https://www.parallax.com/product/28966 | Parallax Arlo Complete Robot System]] together this
week. I now have all of the major components built and wired up. The Power Distribution Board part of the kit was
missing its PCB mounted DC Power Jack for connection to the battery charger. I have pinged Parallax about the missing
component and I will complete the build once I have tracked down a replacement. I plan to use the included
[[https://www.parallax.com/product/32912 | Parallax Propeller Activity Board WX]] to perform some basic system checks in
the meantime.



==June 20th, 2016
===What I learned from Robo-Magellan 2015
I went over to Seattle Center to watch the re-run of the Robo-Magellan 2015 competition on Sunday. It was beautiful
weather for the event. Four robot builders took part in the competition: Will Smith, Bob Cook, Charles Glorioso, and
Dave Coleman. All four of the bots got within viewing distance of the final cone but none managed to actually touch it.
This was still much better results than I have seen in the last few years. The programming in these bots seems to be
getting pretty close.

I did have some takeaways from watching this event that I need to take into account for the design of my robot:
* **Trees and benches**: There are a number of trees and benches close to the Seattle Children's Theater where the goal
  cone is usually placed. I had previously thought that if I had a good dead reckoning system in place, I wouldn't need
  to worry so much about obstacle avoidance but seeing the tights spots that need to be navigated between the trees and
  benches is giving me serious doubts about that plan.\\
  {{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160620-03.jpg}}
* **High Centering**: There are small curbs around the base of the previously mentioned trees. Some of the smaller
  robots got high centered on those during the competition on Sunday.
* **Steep Hill**: There are some steep hills around the Mural Lawn, especially at the northwest corner. The steepness of
  the transition from the lawn to the western walkway decreases as you travel south so that it probably the safer path
  for the robot to take if possible.
* **People**: There have always been some people sitting on the Mural Lawn during the competition but in past years,
  they would move if they saw the self-driving robots approaching them. The weather was so nice on Sunday that there
  were several sunbathers on the lawn and about half of them didn't want to move when the bot approached. There were a
  few times that the clock was stopped and the robot owner allowed to move the bot around the people who wouldn't move.
  I think adding some flashing lights and the sound of a truck backing up might be a good addition to my robot.

===Kalman Filter Ported to LPC1768
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160620-04.gif}}\\
I completed the initial port of the Kalman filter to the LPC1768 last week. This work included the following commits:
* [[https://github.com/adamgreen/Ferdinand16/commit/f32d39b205e7502e0045e3c9fd175c8fe6daa55b | Start porting orientation code to LPC1768]]
* [[https://github.com/adamgreen/Ferdinand16/commit/2b74c0fe1629f844e95034af1a90e8ee95ea3424 | Calculate orientation quaternion from accelerometer/magnetometer]]
* [[https://github.com/adamgreen/Ferdinand16/commit/0c7a3099b27d1fabef68079ff92be56932b5c9b1 | Swizzle accel/mag based on config.ini settings]]
* [[https://github.com/adamgreen/Ferdinand16/commit/b99cfe091d14ba7c803ebec44c9c9f60a8ff8bfe | Kalman filter ported to LPC1768]]

I had originally had worries that the floating point math required for running the Kalman filter would be too much to
run it on the LPC1768. It turns out that this worry wasn't necessary since the ported Kalman filter ended up
using **<11% of CPU resources**.

This ported code appears to work as expected when switching the orientation.pde sample into Embedded mode (by pressing
the E key). The orientation sample does issue println()s though indicating that there are some differences in the PC and
embedded calculated orientation quaternions until the filter converges. The orientation quaternions calculated from gyro
rate integration and accelerometer/magnetometer fusion on the PC and LPC1768 match within 1.0E-5. It is just the Kalman
filter results that sometimes differ. I will start investigating these differences this week. One theory I have is that
the PC version gets more precision by using special multiply-accumulate or dot-product instructions.

===Parallax Arlo Has Arrived!
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160620-02.jpg}}
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160620-01.jpg}}\\
My Parallax Arlo robot platform arrived last week and I plan to spend some time this week putting it together. Once I
have it built, I can start experimenting with the motor controller and wheel encoders. Putting this together with the
Kalman filtered orientation sensor should let me start evaluating my dead-reckoning strategy.



==June 19th, 2016
===Robo-Magellan 2015
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160619-01.jpg}}\\
The 2015 Robo-Magellan competition was delayed due to the poor weather experienced at last year's Robothon event. It was
re-scheduled and run at Seattle Center today. It was a beautiful day for running bots at the base of the Space Needle.

The first and second place competitors came up from San Francisco and Bob Cook came down from Vancouver, BC for the
event. The fourth competitor was Will Smith, who is from the Seattle area. None of the competitors actually tagged the
final cone but all four competitors had runs where they got pretty close.

===1st Place
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160619-02.jpg}}\\
First place went to Charles Glorioso with the robot shown in the above photo. The animated image below shows his bot
tagging the 0.8 bonus code during its first run:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160619-06.gif}}\\

===2nd Place
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160619-03.jpg}}\\
Dave Coleman won second place with the robot shown in the above photo.

===3rd Place
Will Smith won third place with this robot:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160619-04.jpg}}\\

===Bob Cook
Bob Cook also competed today with this fine robot:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160619-05.jpg}}\\



==June 13th, 2016
===ESP8266 Configured for Remote Debugging/Programming
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160612-02.jpg}}\\
I now have the [[https://github.com/jeelabs/esp-link | JEELABS' esp-Link firmware]] running on my
[[https://www.adafruit.com/product/2821 | ESP8266 board]] so that I can use it for remote debugging of a
[[https://developer.mbed.org/platforms/mbed-LPC1768/ | mbed-LPC1768]] over WiFi. I can also use my
[[https://github.com/adamgreen/bb-8/tree/master/mriprog | mriprog utility]] to remotely program the mbed device as well.
The photo above shows it all setup with GDB running on my MacBook Air connected to the mbed device via WiFi. The USB
connection was just being used for power.

====Removal of Charging Circuit
One nice thing about the Adafruit Feather HUZZAH with ESP8266 WiFi board is that it has a USB to serial device already
on the board so that I didn't have to worry about getting that serial connection all setup so that I could program the
esp-Link firmware into the ESP8266. This USB to serial device is configured so that esptool can use the RTS and DTR
signals to automatically reset the ESP8266 into boot loader mode. This meant I didn't even need to figure out which pins
to pull low at which time.

One thing I didn't like about this particular board for this project was the built-in LiPo battery charging support. I
have no use for this functionality in this particular project and it might even cause problems since I will be
connecting the robot's main power supply to the VBAT pin and I never want this LiPo charging circuit to attempt to
charge those batteries. I decided to disable this charging functionality on the board to be on the safe side.

I first cut the trace between the two pads marked SJ1 on the
[[https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/docs/AdafruitFeatherHuzzahESP8266Schematic.png | schematic]].
These pads are circled yellow in the image below:\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160612-01.jpg}}\\
After I cut that trace, the charging LED would randomly blink when powered from VBAT. It appears that the MCP7381/2
charging chip didn't like the VBAT pin to be floating when its VDD pin was powered from the battery and not USB. My
solution to that problem was to also remove the MCP7381/2 chip, circled red in the image.

===Status as of June 13th
My work items for last week were:
* Simplify and port IMU code.
** I read Phil Kim's [[http://www.amazon.com/Kalman-Filter-Beginners-MATLAB-Examples/dp/1463648359 | "Kalman Filter for Beginners with MATLAB Examples]] and [[http://www.amazon.com/Rigid-Body-Dynamics-Beginners-Quaternions/dp/1493598201 | "Rigid Body Dynamics For Beginners: Euler angles & Quaternions"]] books to research the idea of using a simpler version of a Kalman or complementary filter based on Euler angles instead of the current quaternion solution but the more I read, the more I like the idea of sticking with the current solution. 
** At this point I don't even know how expensive the current quaternion version of the Kalman filter is to run on the LPC1768 so I would like to start porting the existing Processing code to begin performance measurements.
* Setup ESP8266 for remote debugging/programming of LPC1768.
** This work item was completed!

My work items for the week ahead are:
* Start porting the existing quaternion based Kalman filter Processing code running on my MacBook Air to C/C++ code running on the LPC1768 microcontroller.
** Determine just how expensive this calculation is to run on the microcontroller and the optimizations upon which I should really concentrate my efforts.
* Attend the rerunning of the SRS Robo-Magellan 2015 competition on June 19th at the Seattle Center. The original 2015 competition was postponed last fall during Robothon due to bad weather. I will just be a spectator at this event.



==June 10th, 2016
===Lots of Fun Reading
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160606-01.jpg}}\\
I finished re-reading
[[http://www.amazon.com/Kalman-Filter-Beginners-MATLAB-Examples/dp/1463648359 | Phil Kim's "Kalman Filter for Beginners with MATLAB Examples]]
this week. I now have a basic understanding of Kalman filters again and I spent a bit more energy this time reading the
chapter on complementary filters as well since I just skimmed it last time. I think I now know enough about
complementary filters to be able to implement one in C/C++. The only thing is that it would be for a Euler angle
representation.  I prefer the no singularity aspect of my current quaternion based Kalman filter implementation.

{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160607-01.jpg}}\\
As I was reading Phil Kim's Kalman Filter book, I thought I should continue the fun of understanding how to represent
the orientation of objects in 3D space by reading his other book,
[[http://www.amazon.com/Rigid-Body-Dynamics-Beginners-Quaternions/dp/1493598201 | Rigid Body Dynamics For Beginners: Euler angles & Quaternions]].
Euler Angles and Quaternions!  What Fun! It arrived on Tuesday, June 7th, and I started reading it the next evening
after I had completed my Kalman filter reading. Like his Kalman filter book, there is a lot of great information in this
book without getting bogged down too much in heavy mathematics (except for maybe the last 2 chapters). Unlike his Kalman
filter book, the translation to English is actually very good in this book. It doesn't have the awkward sentence
structure found in the other. I highly recommend it for understanding how to describe the motion of rigid bodies, like
Robo-Magellan competitors, in 3D space.

===New Toys from Sparkfun
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160609-01.jpg}}\\
Even before I started on this Ferdinand16 project, I had already made a Sparkfun order which included some more
[[https://developer.mbed.org/platforms/mbed-LPC1768/ | mbed LPC1768 boards]] since my stockpile was running low. I also
added a breadboard to the order due to the fact that all of my existing ones were being used by other in progress
projects. The main reason for the timing of the order was that I had
[[https://github.com/mbedmicro/mbed/pull/1776 | issued a pull request to the mbed SDK]] for a bug I had encountered in
the I2C driver for NXP LPC* parts and originally it was indicated that I had broken existing tests which used the
Microchip 256kbit 24LC256 I2C EEPROM. I placed an order for one from Sparkfun to use for reproducing the bug and then I
added those other goodies to the order to justify the shipping costs. It turned out that the issue with my PR was just
with the way my PR was merged and not a bug in my actual fix. But now that the order has arrived, I will put one of the
mbed LPC1768 boards and the breadboard to good use. I will use them to get my
[[https://www.adafruit.com/product/2821 | ESP8266 board]] setup and running
[[https://github.com/jeelabs/esp-link | JEELABS' esp-Link firmware]] to allow remote programming and debugging of the
LPC1768.



==June 6th, 2016
===IMU Updates
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160606-01.jpg}}\\
I am re-reading
[[http://www.amazon.com/Kalman-Filter-Beginners-MATLAB-Examples/dp/1463648359 | Phil Kim's "Kalman Filter for Beginners with MATLAB Examples]]
to refresh my memory on both Kalman and Complementary filters for fusing accelerometer/magnetometer and gyro sensor
measurements. Once I finish with that research I plan to simplify the sensor fusion algorithm that I used on
[[https://github.com/adamgreen/Ferdinand14#readme | Ferdinand14]] and port it to run on the LPC1768 microcontroller
directly instead of on the PC as it does currently.

===Adding WiFi
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160606-02.jpg}}\\
My [[https://www.adafruit.com/product/2821 | ESP8266 board from Adafruit]] arrived today in the mail. I plan to get the
[[https://github.com/jeelabs/esp-link | JEELABS' esp-Link firmware]] installed and tested on this board later in the
week. I used that same firmware for
[[https://github.com/adamgreen/bb-8#wireless-debuggingprogramming-using-esp8266 | remote debugging and programming]]
my BB-8 ball bot and it was really useful. I want to replicate that experience for this project as well.

So those are my work items for this week:
* Simplify and port IMU code.
* Setup ESP8266 for remote debugging/programming of LPC1768.



==June 3rd, 2016
It has been 2 years since I helped [[https://github.com/Xandon | Xandon]] out with his
[[http://www.robothon.org/robothon/robo-magellan.php | Robo-Magellan]] robot competitor. I took a well needed break away
from the Robo-Magellan competition last year. I had planned to do the same this year but ideas for a new Robo-Magellan
robot kept resurfacing in my mind. I am creating this **Ferdinand16** project to track these ideas and my attempt to
bring them to life in time for the [[http://www.robothon.org/robothon/index.php | 2016 SRS Robothon]].

===The Ideas
I learned a lot from working with Xandon back in 2014 on his Robo-Magellan robot. Many of the ideas for my own
Robo-Magellan robot come from that great experience.

Let's start with my ideas for the hardware.  Where possible, I plan to select hardware that will make the software
simpler:
* **Larger & Heavier:** Many of the other competitors used RC trucks as their base. These smaller bots make for easy
  transportation to testing and competition sites. The larger, heavier robots like the one Xandon used provide a more
  stable (less vibration) platform for various sensors like cameras and IMUs. The increased height also tends to offer a
  better vantage point for the sensors as well.
* **Differential Wheeled:** The differential wheeled robot is just so much easier to control from software. Their
  turning radius is so tight that they can turn in-place. The ackerman steering used with RC trucks tend to have larger
  turning radii and often require reverse motion to get out of tight situations. Even course correction while
  translating from one traffic cone to the next tends to be easier with the differential robots since they can change
  heading easily no matter how slow they are currently traveling.
* **Wheel Encoders:** Having encoders on the wheels will provide the software with more information about the distance
  it has translated since the last waypoint or cone.
* **Accurate Heading Sensor:** I plan to re-use the orientation sensor from our
  [[https://github.com/adamgreen/Ferdinand14 | 2014 attempt]]. We had the 9 DoF inertial measurement unit in that
  project working very well for the 2014 competition and I want to leverage it for this year's as well.
* **No GPS:** From what I have seen in robots created by others for previous Robo-Magellan competitions, writing
  software which can filter the GPS data to navigate the Seattle Center course reliably can prove to be quite difficult.
  I want to skip that difficulty if possible.
* **Microcontrollers, Not Microprocessors:** I want to constrain myself to running all of the software for this robot on
  microcontrollers rather than resorting to higher end microprocessor based systems. This forces me to use the KISS
  (Keep It Simple Stupid) principles when designing and implementing my software. Having something like a laptop on my
  robot would allow me to potentially go overboard and try to do too much like using sophisticated computer vision and
  localization algorithms. This could lead to some very tricky issues that I couldn't solve by October 1st.
* **Off-the-Shelf Vision System:** Related to the previous item, I want to use an existing proven computer vision system
  if possible so that I can instead concentrate on writing the rest of the navigation software.

Giving myself the above hardware constraints, I can start to consider the high level goals of the software:
* **Filtered Compass Heading:** I plan to take the
  [[https://github.com/adamgreen/Ferdinand14/tree/master/orientation | IMU code]] that I wrote for the 2014 competition
  and simplify it. That code uses a Kalman filter to determine the robot's complete 3D orientation when only the compass
  heading is required. I have some ideas on how to simplify that code to only return the compass heading and run on an
  ARM Cortex-M processor rather than a PC as was done before.
* **Dead reckoning navigation:** I plan to take the information from the IMU mentioned above and combine it with the
  wheel odometry to use dead reckoning as the method to navigate between waypoints and traffic cones. The math for this
  is quite simple. The complexity will be the noise and errors that the real world throws into the works.
* **Cone Finding:** I plan to use a
  [[http://charmedlabs.com/default/pixy-cmucam5/ | Pixy (CMUcam5) camera from Charmed Labs]] to keep the robot on target
  by tracking the traffic cone once the dead reckoning gets the robot close enough.

===Initial Build of Materials
Based on the ideas I have so far for this robot, I have decided to use the following hardware for my build:
* [[https://github.com/adamgreen/Ferdinand14/tree/master/orientation | Ferdinand14 IMU]]: I plan to re-use the IMU
  hardware from the 2014 attempt. I will simplify the code and port it to run on ARM Cortex-M embedded hardware instead
  of the PC. //In the image below, the black box on the right contains the IMU hardware and the MacBook Air on the left
  is running the Kalman filter to calculate and display the IMU's current orientation.//\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-05.jpg}}
* [[https://www.parallax.com/product/arlo-robotic-platform-system | Parallax Arlo Platform]]: Rather than take the time
  to build a robot platform that meets my needs, I have decided to go with the Arlo Robotic Platform System from
  Parallax. The complete kit even comes with the motors, wheel encoders, motor drivers, and batteries that I need to
  jump start this project.\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-01.png}}
* [[http://charmedlabs.com/default/pixy-cmucam5/ | Pixy camera from Charmed Labs]]: This camera includes its own dual
  core ARM Cortex-M4F & Cortex-M0 microprocessor to track colour blobs like bright orange traffic cones. In 2014
  [[https://github.com/adamgreen/Ferdinand14#vision-based-traffic-cone-detector | I implemented a similar blob detector]]
  to run on the PC using the RGB video output from the Kinect camera. This time I will leverage this off the shelf
  solution from Charmed Labs.\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-04.jpg}}
* [[https://developer.mbed.org/platforms/mbed-LPC1768/ | mbed NXP LPC1768 96MHz Cortex-M3 Board]]: This is the
  microcontroller with which I am most familiar so I plan to leverage that experience for this project. It has 512K of
  FLASH and 64K of RAM.\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-06.jpg}}
* [[https://www.adafruit.com/product/2821 | Adafruit Feather HUZZAH with ESP8266 WiFi]]: The ESP8266 is a really cool
  little microcontroller on its own. It supports WiFi communications in a <$10 chip. I plan to use this as a UART to
  WiFi bridge like I did in my
  [[https://github.com/adamgreen/bb-8#wireless-debuggingprogramming-using-esp8266 | BB-8 replica project]]. This will
  allow me to remotely debug the LPC1768 using GDB and also remotely program it with my
  [[https://github.com/adamgreen/bb-8/tree/master/mriprog | mriprog utility]].\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-02.jpg}}

Earlier this week I placed orders for the above items which I didn't already have in my possession.

===Pixy Camera
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-04.jpg}}\\
I dropped the other work on the project I was doing when the Pixy camera arrived on Wednesday and started to experiment
with it instead. I installed the latest [[http://cmucam.org/projects/cmucam5/wiki/Installing_PixyMon_on_Mac_2 | PixyMon]]
on my MacBook Air, dusted off my orange traffic cone last used in 2014, and headed outside on an overcast afternoon to
experiment with the Pixy. I have a few first impressions that I will document here now but I know that I still have much
to learn about this powerful tool:
* It was easy to train the Pixy to learn the colour signature for the traffic cone. The camera would then do a good job
  of tracking the cone **until**:
** I restarted the camera. I suspect that different restarts of the camera would tune for white balance differently and
   that would impact the quality of the previously recorded signature. I can see 2 solutions for this:
### Tilt the camera down to look at a white test patch affixed to the robot during power-up so that it tunes to the same
    white pattern each time.
### Find good white balance settings for outdoors and fix the camera to use those settings if possible. This might
    require modification to the Pixy firmware.
** I observed the cone from a different angle. Usually I would see the quality of the tracking degrade right after it
   was obvious that the camera had adjusted its exposure time due to a change in lighting conditions.

I also ordered a [[http://charmedlabs.com/default/pan-tilt-kit/ | Pan-Tilt Kit from Charmed Labs]]. I don't know if I
will end up using this on my final robot or not. I thought it might prove useful so I ordered it along with the camera
just incase. The documentation for the Pixy did mention that the servos might not work correctly if powering the Pixy
from too long of a USB cable. In my case, the device would just reset whenever I attempted to put the camera into
pan-tilt tracking mode. I switched to a short cable (~1 foot) and it worked flawlessly.

===Ferdinand14 IMU
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-05.jpg}}\\

I pulled the IMU hardware and software from the 2014 attempt out of storage and started to experiment with it to
familiarize myself with it again. I quickly got it back up and running
[[https://github.com/adamgreen/Ferdinand14/tree/master/orientation | the orientation sample]] from my
[[https://github.com/adamgreen/Ferdinand14#readme | Ferdinand14 project]]. It was cool to see the unfiltered pose
estimation, calculated directly from raw accelerometer and magnetometer sensor readings, rendered on the screen with all
of its susceptibility to vibrations & errors and then switch to Kalman filtered mode to see the nice smooth sensor pose
being rendered instead. This sample also has the ability to switch into a mode which just uses gyro rate integration to
track the orientation. When I switched into this mode I found that the yaw and pitch axis would not show perceptible
drift but the roll axis would generate so much drift that it would visibly rotate around this axis in the rendered
video. I reran the [[https://github.com/adamgreen/Ferdinand14#sensor-calibration | IMU calibration process]] and did
find that one gyro now required different compensation constants for reducing temperature induced zero bias. The first
graph below is from this week's calibration run while the second one is from back in 2014. The equations in the upper
left hand portion of the graph give the equation of the linear trend lines that have been drawn through the gathered
points. The slopes of all of these trend lines are still pretty much the same as back in 2014 and so are most of the
y-intercepts except for the first one which had changed from 52.447 to 48.138.\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-03.png}}\\
{{https://raw.githubusercontent.com/adamgreen/Ferdinand16/master/photos/20160603-07.png}}\\

I updated the calibration settings in the config file for this y-intercept and then the orientation sample had no
problems maintaining a steady orientation pose from gyro rate integration. I could leave the device sitting on the table
for >5 minutes and still not notice any perceptible change in the rendered object's pose.

===Clone this Repo and its Submodules
Cloning now requires a few more options to fetch all of the necessary code (including GCC4MBED).

{{{
git clone --recursive git@github.com:adamgreen/Ferdinand16.git
}}}

* In the gcc4mbed subdirectory you will find multiple install scripts.  Run the install script appropriate for your
platform:
** Windows: win_install.cmd
** OS X: mac_install
** Linux: linux_install
* You can then run the BuildShell script which will be created during the install to properly configure the PATH
environment variable.  You may want to edit this script to further customize your development environment.

If you encounter any issues with the install then refer to the **Important Notes** section of the README in the gcc4mbed
subdirectory.
